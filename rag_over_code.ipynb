{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## going through the code base is important and daunting too, understanding how everything works and knowing what sorts of function and classes is used is important\n",
    "## hence, we will be building a RAG based pipeline on code bases / code repo\n",
    "\n",
    "## github co-pilot, codeium are all steroid versions of these "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Same vanilla RAG pipeline, but we use ContextAwareSplitting in RecursiveCharacterTextSplitting by specifying language of the codebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How context aware works \n",
    "\n",
    "# In particular, we can employ a splitting strategy that does a few things:\n",
    "\n",
    "# Keeps each top-level function and class in the code is loaded into separate documents.\n",
    "# Puts remaining into a separate document.\n",
    "# Retains metadata about where each split comes from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain tiktoken chromadb  openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<git.repo.base.Repo '/mnt/e/tinkering/github/tinkering_RAG/code_repo/.git'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's clone a repo and use that to test and create my RAG \n",
    "# will clone my ChatAI proj and will use that \n",
    "from git import Repo\n",
    "\n",
    "code_repo_path = r\"/mnt/e/tinkering/github/tinkering_RAG/code_repo\"\n",
    "Repo.clone_from(\"https://github.com/MANISH007700/ChatAI.git\", to_path = code_repo_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libs and pkges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter # this will help us split data \n",
    "from langchain.text_splitter import Language   # this will help us decide the language\n",
    "\n",
    "from langchain.document_loaders.generic import GenericLoader    # kind of like AutoModel from HF, helps in loading generic stuff by passing additional kwargs\n",
    "from langchain.document_loaders.parsers import LanguageParser   # kind of parser you wanna use in GenericLoader "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse the code repo and load it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = GenericLoader.from_filesystem(\n",
    "    r\"/mnt/e/tinkering/github/tinkering_RAG/code_repo\",\n",
    "    glob = \"*\",\n",
    "    suffixes = [\".py\"],  # only python files [ as of now, python and js is supported by LanguageParser],\n",
    "    parser = LanguageParser(language=Language.PYTHON, parser_threshold=30)   # parse_threshold : min number of code lines for the parser to split into text chunks\n",
    ")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='def main():\\n\\n    st.set_page_config(page_title=\"Chat with multiple PDFs\", page_icon=\":books:\")\\n    st.write(css, unsafe_allow_html=True)\\n\\n    if \"conversation\" not in st.session_state:\\n        st.session_state.conversation = None\\n    if \"chat_history\" not in st.session_state:\\n        st.session_state.chat_history = None\\n\\n    with st.sidebar:\\n        st.subheader(\"Your documents\")\\n        pdf_docs = st.file_uploader(\"Upload your PDFs, API KEY here and click on \\'Process\\'\", accept_multiple_files=True)\\n        openai_api_key = st.text_input(\"OPENAI API KEY\", key=\"file_qa_api_key\", type=\\'password\\')\\n        \\n        if st.button(\"Process\"):\\n            with st.spinner(\"Processing\"):\\n                \\n                # get pdf text\\n                raw_text = get_pdf_text(pdf_docs)\\n                st.text(\"Extracting Texts Done ✅\")\\n\\n                # get the text chunks\\n                text_chunks = get_text_chunks(raw_text)\\n                st.text(\"Processing Chunks Done ✅\")\\n                \\n                # create vector store\\n                vectorstore = get_vectorstore(text_chunks, openai_api_key)\\n                st.text(\"Memory loaded Done ✅\")\\n                \\n                # create conversation chain\\n                st.session_state.conversation = get_conversation_chain(vectorstore, openai_api_key)\\n\\n\\n    st.header(\"Chat with multiple PDFs :books:\")\\n    user_question = st.text_input(\"Start chatting with your docs.\")\\n\\n    if not user_question:\\n        st.info(\"Please upload docs and openai key, and then proceed to chat\")\\n\\n    if user_question and not openai_api_key:\\n        st.info(\"Please add openai key \")\\n    \\n    if user_question and openai_api_key: \\n        handle_userinput(user_question)', metadata={'source': '/mnt/e/tinkering/github/tinkering_RAG/code_repo/app.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content=\"import streamlit as st\\nimport openai\\nfrom loguru import logger \\nfrom dotenv import load_dotenv\\n\\nfrom utils import get_pdf_text\\nfrom utils import get_text_chunks\\nfrom utils import get_vectorstore\\nfrom utils import get_conversation_chain\\nfrom utils import handle_userinput\\n\\nfrom htmlTemplates import css\\n\\n\\n# Code for: def main():\\n\\n\\n\\nif __name__ == '__main__':\\n    main()\", metadata={'source': '/mnt/e/tinkering/github/tinkering_RAG/code_repo/app.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='css = \\'\\'\\'\\n<style>\\n.chat-message {\\n    padding: 1.5rem; border-radius: 0.5rem; margin-bottom: 1rem; display: flex\\n}\\n.chat-message.user {\\n    background-color: #2b313e\\n}\\n.chat-message.bot {\\n    background-color: #475063\\n}\\n.chat-message .avatar {\\n  width: 20%;\\n}\\n.chat-message .avatar img {\\n  max-width: 78px;\\n  max-height: 78px;\\n  border-radius: 50%;\\n  object-fit: cover;\\n}\\n.chat-message .message {\\n  width: 80%;\\n  padding: 0 1.5rem;\\n  color: #fff;\\n}\\n\\'\\'\\'\\n\\nuser_template = \\'\\'\\'\\n<div class=\"chat-message user\">\\n    <div class=\"avatar\">\\n        <img src=\"https://w7.pngwing.com/pngs/400/768/png-transparent-laptop-computer-icons-user-drawing-computer-user-miscellaneous-blue-rectangle-thumbnail.png\" style=\"max-height: 78px; max-width: 78px; border-radius: 50%; object-fit: cover;\">\\n    </div>    \\n    <div class=\"message\">{{MSG}}</div>\\n</div>\\n\\'\\'\\'\\n\\nbot_template = \\'\\'\\'\\n<div class=\"chat-message bot\">\\n    <div class=\"avatar\">\\n        <img src=\"https://upload.wikimedia.org/wikipedia/commons/8/88/Mini-Robot.png\" style=\"max-height: 78px; max-width: 78px; border-radius: 50%; object-fit: cover;\">\\n    </div>\\n    <div class=\"message\">{{MSG}}</div>\\n</div>\\n\\'\\'\\'', metadata={'source': '/mnt/e/tinkering/github/tinkering_RAG/code_repo/htmlTemplates.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='def get_pdf_text(pdf_docs):\\n    text = \"\"\\n    for pdf in pdf_docs:\\n        pdf_reader = PdfReader(pdf)\\n        for page in pdf_reader.pages:\\n            text += page.extract_text()\\n    logger.info(\"Text Extracted...\")\\n    return text', metadata={'source': '/mnt/e/tinkering/github/tinkering_RAG/code_repo/utils.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='def get_text_chunks(text):\\n    text_splitter = CharacterTextSplitter(\\n        separator=\"\\\\n\",\\n        chunk_size=1000,\\n        chunk_overlap=200,\\n        length_function=len\\n    )\\n    chunks = text_splitter.split_text(text)\\n    logger.info(\"Chunks made..\")\\n    return chunks', metadata={'source': '/mnt/e/tinkering/github/tinkering_RAG/code_repo/utils.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='def get_vectorstore(text_chunks, api_key=None):\\n    embeddings = OpenAIEmbeddings(openai_api_key = api_key)\\n\\n    # If you wanna use HF Embedding model, uncomment this and comment out OpenAIEmbeddings()\\n    # embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\")\\n    \\n    vectorstore = FAISS.from_texts(texts=text_chunks, embedding=embeddings)\\n    logger.info(\"Embeddings are created..\")\\n    return vectorstore', metadata={'source': '/mnt/e/tinkering/github/tinkering_RAG/code_repo/utils.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='def get_conversation_chain(vectorstore, api_key=None):\\n    llm = ChatOpenAI(openai_api_key = api_key)\\n\\n    # if you wanna use HF models, uncomment this and comment out ChatOpenAI()\\n    # llm = HuggingFaceHub(repo_id=\"google/flan-t5-xxl\", model_kwargs={\"temperature\":0.5, \"max_length\":512})\\n\\n    memory = ConversationBufferMemory(memory_key=\\'chat_history\\', return_messages=True)\\n    conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=vectorstore.as_retriever(), memory=memory)\\n    return conversation_chain', metadata={'source': '/mnt/e/tinkering/github/tinkering_RAG/code_repo/utils.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='def handle_userinput(user_question):\\n    response = st.session_state.conversation({\\'question\\': user_question})\\n    \\n    # logger.info(\"All response ..... \")\\n    # logger.info(response)\\n\\n    st.session_state.chat_history = response[\\'chat_history\\']\\n\\n    for i, message in enumerate(st.session_state.chat_history):\\n        if i % 2 == 0:\\n            st.write(user_template.replace(\\n                \"{{MSG}}\", message.content), unsafe_allow_html=True)\\n        else:\\n            st.write(bot_template.replace(\\n                \"{{MSG}}\", message.content), unsafe_allow_html=True)', metadata={'source': '/mnt/e/tinkering/github/tinkering_RAG/code_repo/utils.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import streamlit as st \\n\\nfrom loguru import logger \\nfrom PyPDF2 import PdfReader\\n\\nfrom langchain.text_splitter import CharacterTextSplitter\\nfrom langchain.embeddings import OpenAIEmbeddings, HuggingFaceInstructEmbeddings\\nfrom langchain.vectorstores import FAISS\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.memory import ConversationBufferMemory\\nfrom langchain.chains import ConversationalRetrievalChain\\nfrom htmlTemplates import bot_template, user_template\\nfrom langchain.llms import HuggingFaceHub\\n\\n\\n# Code for: def get_pdf_text(pdf_docs):\\n\\n\\n# Code for: def get_text_chunks(text):\\n\\n\\n# Code for: def get_vectorstore(text_chunks, api_key=None):\\n\\n\\n# Code for: def get_conversation_chain(vectorstore, api_key=None):\\n\\n\\n# Code for: def handle_userinput(user_question):', metadata={'source': '/mnt/e/tinkering/github/tinkering_RAG/code_repo/utils.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>})]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the docs based on language using RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter.from_language(language=Language.PYTHON, chunk_size = 1000, chunk_overlap = 200)\n",
    "split_docs = text_splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Document(page_content='def main():\\n\\n    st.set_page_config(page_title=\"Chat with multiple PDFs\", page_icon=\":books:\")\\n    st.write(css, unsafe_allow_html=True)\\n\\n    if \"conversation\" not in st.session_state:\\n        st.session_state.conversation = None\\n    if \"chat_history\" not in st.session_state:\\n        st.session_state.chat_history = None\\n\\n    with st.sidebar:\\n        st.subheader(\"Your documents\")\\n        pdf_docs = st.file_uploader(\"Upload your PDFs, API KEY here and click on \\'Process\\'\", accept_multiple_files=True)\\n        openai_api_key = st.text_input(\"OPENAI API KEY\", key=\"file_qa_api_key\", type=\\'password\\')\\n        \\n        if st.button(\"Process\"):\\n            with st.spinner(\"Processing\"):\\n                \\n                # get pdf text\\n                raw_text = get_pdf_text(pdf_docs)\\n                st.text(\"Extracting Texts Done ✅\")', metadata={'source': '/mnt/e/tinkering/github/tinkering_RAG/code_repo/app.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n",
       "  Document(page_content='# get the text chunks\\n                text_chunks = get_text_chunks(raw_text)\\n                st.text(\"Processing Chunks Done ✅\")\\n                \\n                # create vector store\\n                vectorstore = get_vectorstore(text_chunks, openai_api_key)\\n                st.text(\"Memory loaded Done ✅\")\\n                \\n                # create conversation chain\\n                st.session_state.conversation = get_conversation_chain(vectorstore, openai_api_key)\\n\\n\\n    st.header(\"Chat with multiple PDFs :books:\")\\n    user_question = st.text_input(\"Start chatting with your docs.\")\\n\\n    if not user_question:\\n        st.info(\"Please upload docs and openai key, and then proceed to chat\")\\n\\n    if user_question and not openai_api_key:\\n        st.info(\"Please add openai key \")\\n    \\n    if user_question and openai_api_key: \\n        handle_userinput(user_question)', metadata={'source': '/mnt/e/tinkering/github/tinkering_RAG/code_repo/app.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n",
       "  Document(page_content=\"import streamlit as st\\nimport openai\\nfrom loguru import logger \\nfrom dotenv import load_dotenv\\n\\nfrom utils import get_pdf_text\\nfrom utils import get_text_chunks\\nfrom utils import get_vectorstore\\nfrom utils import get_conversation_chain\\nfrom utils import handle_userinput\\n\\nfrom htmlTemplates import css\\n\\n\\n# Code for: def main():\\n\\n\\n\\nif __name__ == '__main__':\\n    main()\", metadata={'source': '/mnt/e/tinkering/github/tinkering_RAG/code_repo/app.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       "  Document(page_content='css = \\'\\'\\'\\n<style>\\n.chat-message {\\n    padding: 1.5rem; border-radius: 0.5rem; margin-bottom: 1rem; display: flex\\n}\\n.chat-message.user {\\n    background-color: #2b313e\\n}\\n.chat-message.bot {\\n    background-color: #475063\\n}\\n.chat-message .avatar {\\n  width: 20%;\\n}\\n.chat-message .avatar img {\\n  max-width: 78px;\\n  max-height: 78px;\\n  border-radius: 50%;\\n  object-fit: cover;\\n}\\n.chat-message .message {\\n  width: 80%;\\n  padding: 0 1.5rem;\\n  color: #fff;\\n}\\n\\'\\'\\'\\n\\nuser_template = \\'\\'\\'\\n<div class=\"chat-message user\">\\n    <div class=\"avatar\">\\n        <img src=\"https://w7.pngwing.com/pngs/400/768/png-transparent-laptop-computer-icons-user-drawing-computer-user-miscellaneous-blue-rectangle-thumbnail.png\" style=\"max-height: 78px; max-width: 78px; border-radius: 50%; object-fit: cover;\">\\n    </div>    \\n    <div class=\"message\">{{MSG}}</div>\\n</div>\\n\\'\\'\\'', metadata={'source': '/mnt/e/tinkering/github/tinkering_RAG/code_repo/htmlTemplates.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       "  Document(page_content='bot_template = \\'\\'\\'\\n<div class=\"chat-message bot\">\\n    <div class=\"avatar\">\\n        <img src=\"https://upload.wikimedia.org/wikipedia/commons/8/88/Mini-Robot.png\" style=\"max-height: 78px; max-width: 78px; border-radius: 50%; object-fit: cover;\">\\n    </div>\\n    <div class=\"message\">{{MSG}}</div>\\n</div>\\n\\'\\'\\'', metadata={'source': '/mnt/e/tinkering/github/tinkering_RAG/code_repo/htmlTemplates.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>}),\n",
       "  Document(page_content='def get_pdf_text(pdf_docs):\\n    text = \"\"\\n    for pdf in pdf_docs:\\n        pdf_reader = PdfReader(pdf)\\n        for page in pdf_reader.pages:\\n            text += page.extract_text()\\n    logger.info(\"Text Extracted...\")\\n    return text', metadata={'source': '/mnt/e/tinkering/github/tinkering_RAG/code_repo/utils.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n",
       "  Document(page_content='def get_text_chunks(text):\\n    text_splitter = CharacterTextSplitter(\\n        separator=\"\\\\n\",\\n        chunk_size=1000,\\n        chunk_overlap=200,\\n        length_function=len\\n    )\\n    chunks = text_splitter.split_text(text)\\n    logger.info(\"Chunks made..\")\\n    return chunks', metadata={'source': '/mnt/e/tinkering/github/tinkering_RAG/code_repo/utils.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n",
       "  Document(page_content='def get_vectorstore(text_chunks, api_key=None):\\n    embeddings = OpenAIEmbeddings(openai_api_key = api_key)\\n\\n    # If you wanna use HF Embedding model, uncomment this and comment out OpenAIEmbeddings()\\n    # embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\")\\n    \\n    vectorstore = FAISS.from_texts(texts=text_chunks, embedding=embeddings)\\n    logger.info(\"Embeddings are created..\")\\n    return vectorstore', metadata={'source': '/mnt/e/tinkering/github/tinkering_RAG/code_repo/utils.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n",
       "  Document(page_content='def get_conversation_chain(vectorstore, api_key=None):\\n    llm = ChatOpenAI(openai_api_key = api_key)\\n\\n    # if you wanna use HF models, uncomment this and comment out ChatOpenAI()\\n    # llm = HuggingFaceHub(repo_id=\"google/flan-t5-xxl\", model_kwargs={\"temperature\":0.5, \"max_length\":512})\\n\\n    memory = ConversationBufferMemory(memory_key=\\'chat_history\\', return_messages=True)\\n    conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=vectorstore.as_retriever(), memory=memory)\\n    return conversation_chain', metadata={'source': '/mnt/e/tinkering/github/tinkering_RAG/code_repo/utils.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n",
       "  Document(page_content='def handle_userinput(user_question):\\n    response = st.session_state.conversation({\\'question\\': user_question})\\n    \\n    # logger.info(\"All response ..... \")\\n    # logger.info(response)\\n\\n    st.session_state.chat_history = response[\\'chat_history\\']\\n\\n    for i, message in enumerate(st.session_state.chat_history):\\n        if i % 2 == 0:\\n            st.write(user_template.replace(\\n                \"{{MSG}}\", message.content), unsafe_allow_html=True)\\n        else:\\n            st.write(bot_template.replace(\\n                \"{{MSG}}\", message.content), unsafe_allow_html=True)', metadata={'source': '/mnt/e/tinkering/github/tinkering_RAG/code_repo/utils.py', 'content_type': 'functions_classes', 'language': <Language.PYTHON: 'python'>}),\n",
       "  Document(page_content='import streamlit as st \\n\\nfrom loguru import logger \\nfrom PyPDF2 import PdfReader\\n\\nfrom langchain.text_splitter import CharacterTextSplitter\\nfrom langchain.embeddings import OpenAIEmbeddings, HuggingFaceInstructEmbeddings\\nfrom langchain.vectorstores import FAISS\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.memory import ConversationBufferMemory\\nfrom langchain.chains import ConversationalRetrievalChain\\nfrom htmlTemplates import bot_template, user_template\\nfrom langchain.llms import HuggingFaceHub\\n\\n\\n# Code for: def get_pdf_text(pdf_docs):\\n\\n\\n# Code for: def get_text_chunks(text):\\n\\n\\n# Code for: def get_vectorstore(text_chunks, api_key=None):\\n\\n\\n# Code for: def get_conversation_chain(vectorstore, api_key=None):\\n\\n\\n# Code for: def handle_userinput(user_question):', metadata={'source': '/mnt/e/tinkering/github/tinkering_RAG/code_repo/utils.py', 'content_type': 'simplified_code', 'language': <Language.PYTHON: 'python'>})],\n",
       " 11)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_docs, len(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def main():\\n\\n    st.set_page_config(page_title=\"Chat with multiple PDFs\", page_icon=\":books:\")\\n    st.write(css, unsafe_allow_html=True)\\n\\n    if \"conversation\" not in st.session_state:\\n        st.session_state.conversation = None\\n    if \"chat_history\" not in st.session_state:\\n        st.session_state.chat_history = None\\n\\n    with st.sidebar:\\n        st.subheader(\"Your documents\")\\n        pdf_docs = st.file_uploader(\"Upload your PDFs, API KEY here and click on \\'Process\\'\", accept_multiple_files=True)\\n        openai_api_key = st.text_input(\"OPENAI API KEY\", key=\"file_qa_api_key\", type=\\'password\\')\\n        \\n        if st.button(\"Process\"):\\n            with st.spinner(\"Processing\"):\\n                \\n                # get pdf text\\n                raw_text = get_pdf_text(pdf_docs)\\n                st.text(\"Extracting Texts Done ✅\")'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gonna do the RetrievalQA now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we gonna use search_type as 'mmr' - max marginal ref [ this will help us remove any duplicated chunks ], top_k = 5 \n",
    "\n",
    "# we need embeddings to create embeds \n",
    "from langchain.embeddings import OpenAIEmbeddings, HuggingFaceBgeEmbeddings\n",
    "\n",
    "# we need some v-db\n",
    "from langchain.vectorstores import Chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24ac4c2209c7471e97ebf6ee8f233f9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading .gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "537c0bc93b764cb4a8882c79c58d982a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading 1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c722f741c22a49568dcad194598159d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading README.md:   0%|          | 0.00/90.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e85ef55c8d934d4e91363fcc36f57f56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffc41119bd77477caeec3b2df4395cf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ce_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1475b9e36cc46d99d265ac99aef8281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89597f87ad64415b86875f865ba75a79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/134M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f49d13c060b4681ae949785084bc860",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)nce_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "763f37d8200940e38bd2544c3824ba78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbe2d5ff984f42d2ab1eccd9091b8bb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b504053679e440cb8a50d06d79026e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "611d715ba45a46f1aaa633933e8fc624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6dfe7bd325e42a58535c835e787f8b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the bge embeddings model \n",
    "\n",
    "model_name = \"BAAI/bge-small-en-v1.5\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "hf = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check some embeddings using this model bge \n",
    "embeds = hf.embed_query(\"how to create a function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass, os \n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = getpass.getpass()    # prompt user to add openai key\n",
    "vector_db = Chroma.from_documents(split_docs, embedding=OpenAIEmbeddings())\n",
    "\n",
    "retriever = vector_db.as_retriever(\n",
    "    search_type = 'mmr',\n",
    "    search_kwargs = {'k': 5},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='def get_pdf_text(pdf_docs):\\n    text = \"\"\\n    for pdf in pdf_docs:\\n        pdf_reader = PdfReader(pdf)\\n        for page in pdf_reader.pages:\\n            text += page.extract_text()\\n    logger.info(\"Text Extracted...\")\\n    return text', metadata={'content_type': 'functions_classes', 'language': 'python', 'source': '/mnt/e/tinkering/github/tinkering_RAG/code_repo/utils.py'}),\n",
       "  0.34997424483299255),\n",
       " (Document(page_content='def get_pdf_text(pdf_docs):\\n    text = \"\"\\n    for pdf in pdf_docs:\\n        pdf_reader = PdfReader(pdf)\\n        for page in pdf_reader.pages:\\n            text += page.extract_text()\\n    logger.info(\"Text Extracted...\")\\n    return text', metadata={'content_type': 'functions_classes', 'language': 'python', 'source': '/mnt/e/tinkering/github/tinkering_RAG/code_repo/utils.py'}),\n",
       "  0.34997424483299255),\n",
       " (Document(page_content='# get the text chunks\\n                text_chunks = get_text_chunks(raw_text)\\n                st.text(\"Processing Chunks Done ✅\")\\n                \\n                # create vector store\\n                vectorstore = get_vectorstore(text_chunks, openai_api_key)\\n                st.text(\"Memory loaded Done ✅\")\\n                \\n                # create conversation chain\\n                st.session_state.conversation = get_conversation_chain(vectorstore, openai_api_key)\\n\\n\\n    st.header(\"Chat with multiple PDFs :books:\")\\n    user_question = st.text_input(\"Start chatting with your docs.\")\\n\\n    if not user_question:\\n        st.info(\"Please upload docs and openai key, and then proceed to chat\")\\n\\n    if user_question and not openai_api_key:\\n        st.info(\"Please add openai key \")\\n    \\n    if user_question and openai_api_key: \\n        handle_userinput(user_question)', metadata={'content_type': 'functions_classes', 'language': 'python', 'source': '/mnt/e/tinkering/github/tinkering_RAG/code_repo/app.py'}),\n",
       "  0.44536256790161133),\n",
       " (Document(page_content='# get the text chunks\\n                text_chunks = get_text_chunks(raw_text)\\n                st.text(\"Processing Chunks Done ✅\")\\n                \\n                # create vector store\\n                vectorstore = get_vectorstore(text_chunks, openai_api_key)\\n                st.text(\"Memory loaded Done ✅\")\\n                \\n                # create conversation chain\\n                st.session_state.conversation = get_conversation_chain(vectorstore, openai_api_key)\\n\\n\\n    st.header(\"Chat with multiple PDFs :books:\")\\n    user_question = st.text_input(\"Start chatting with your docs.\")\\n\\n    if not user_question:\\n        st.info(\"Please upload docs and openai key, and then proceed to chat\")\\n\\n    if user_question and not openai_api_key:\\n        st.info(\"Please add openai key \")\\n    \\n    if user_question and openai_api_key: \\n        handle_userinput(user_question)', metadata={'content_type': 'functions_classes', 'language': 'python', 'source': '/mnt/e/tinkering/github/tinkering_RAG/code_repo/app.py'}),\n",
       "  0.44536256790161133)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_db.similarity_search_with_score('which function will help me to extract text from pdf ? ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='def get_pdf_text(pdf_docs):\\n    text = \"\"\\n    for pdf in pdf_docs:\\n        pdf_reader = PdfReader(pdf)\\n        for page in pdf_reader.pages:\\n            text += page.extract_text()\\n    logger.info(\"Text Extracted...\")\\n    return text', metadata={'content_type': 'functions_classes', 'language': 'python', 'source': '/mnt/e/tinkering/github/tinkering_RAG/code_repo/utils.py'}),\n",
       "  0.75253083823795),\n",
       " (Document(page_content='def get_pdf_text(pdf_docs):\\n    text = \"\"\\n    for pdf in pdf_docs:\\n        pdf_reader = PdfReader(pdf)\\n        for page in pdf_reader.pages:\\n            text += page.extract_text()\\n    logger.info(\"Text Extracted...\")\\n    return text', metadata={'content_type': 'functions_classes', 'language': 'python', 'source': '/mnt/e/tinkering/github/tinkering_RAG/code_repo/utils.py'}),\n",
       "  0.75253083823795),\n",
       " (Document(page_content='# get the text chunks\\n                text_chunks = get_text_chunks(raw_text)\\n                st.text(\"Processing Chunks Done ✅\")\\n                \\n                # create vector store\\n                vectorstore = get_vectorstore(text_chunks, openai_api_key)\\n                st.text(\"Memory loaded Done ✅\")\\n                \\n                # create conversation chain\\n                st.session_state.conversation = get_conversation_chain(vectorstore, openai_api_key)\\n\\n\\n    st.header(\"Chat with multiple PDFs :books:\")\\n    user_question = st.text_input(\"Start chatting with your docs.\")\\n\\n    if not user_question:\\n        st.info(\"Please upload docs and openai key, and then proceed to chat\")\\n\\n    if user_question and not openai_api_key:\\n        st.info(\"Please add openai key \")\\n    \\n    if user_question and openai_api_key: \\n        handle_userinput(user_question)', metadata={'content_type': 'functions_classes', 'language': 'python', 'source': '/mnt/e/tinkering/github/tinkering_RAG/code_repo/app.py'}),\n",
       "  0.6850811081501165),\n",
       " (Document(page_content='# get the text chunks\\n                text_chunks = get_text_chunks(raw_text)\\n                st.text(\"Processing Chunks Done ✅\")\\n                \\n                # create vector store\\n                vectorstore = get_vectorstore(text_chunks, openai_api_key)\\n                st.text(\"Memory loaded Done ✅\")\\n                \\n                # create conversation chain\\n                st.session_state.conversation = get_conversation_chain(vectorstore, openai_api_key)\\n\\n\\n    st.header(\"Chat with multiple PDFs :books:\")\\n    user_question = st.text_input(\"Start chatting with your docs.\")\\n\\n    if not user_question:\\n        st.info(\"Please upload docs and openai key, and then proceed to chat\")\\n\\n    if user_question and not openai_api_key:\\n        st.info(\"Please add openai key \")\\n    \\n    if user_question and openai_api_key: \\n        handle_userinput(user_question)', metadata={'content_type': 'functions_classes', 'language': 'python', 'source': '/mnt/e/tinkering/github/tinkering_RAG/code_repo/app.py'}),\n",
       "  0.6850811081501165)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_db.similarity_search_with_relevance_scores('which function will help me to extract text from pdf ? ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "llm = ChatOpenAI(model_name = 'gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(llm=llm, memory_key='chat_history', return_messages=True)\n",
    "qa = ConversationalRetrievalChain.from_llm(llm=llm, retriever = retriever, memory = memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'which function will help me to extract text from pdf ? ',\n",
       " 'chat_history': [HumanMessage(content='which function will help me to extract text from pdf ? '),\n",
       "  AIMessage(content='The function that will help you extract text from PDF is the `get_pdf_text` function.')],\n",
       " 'answer': 'The function that will help you extract text from PDF is the `get_pdf_text` function.'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa('which function will help me to extract text from pdf ? ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sure! Here\\'s the code for the `get_pdf_text` function:\\n\\n```python\\ndef get_pdf_text(pdf_docs):\\n    text = \"\"\\n    for pdf in pdf_docs:\\n        pdf_reader = PdfReader(pdf)\\n        for page in pdf_reader.pages:\\n            text += page.extract_text()\\n    logger.info(\"Text Extracted...\")\\n    return text\\n```\\n\\nThis function takes a list of PDF documents as input (`pdf_docs`) and extracts the text from each page of each PDF document using the `extract_text` method from the `PdfReader` class in the `PyPDF2` library. The extracted text is then concatenated and returned as a single string.\\n\\nPlease note that you will need to have the `PyPDF2` library installed in order to use this function.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa('Can you share me which file and the code for get_pdf_text function')['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\" 'Sure! Here's the code for the `get_pdf_text` function:\\n\"\n",
      " '\\n'\n",
      " '```python\\n'\n",
      " 'def get_pdf_text(pdf_docs):\\n'\n",
      " '    text = \"\"\\n'\n",
      " '    for pdf in pdf_docs:\\n'\n",
      " '        pdf_reader = PdfReader(pdf)\\n'\n",
      " '        for page in pdf_reader.pages:\\n'\n",
      " '            text += page.extract_text()\\n'\n",
      " '    logger.info(\"Text Extracted...\")\\n'\n",
      " '    return text\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " 'This function takes a list of PDF documents as input (`pdf_docs`) and '\n",
      " 'extracts the text from each page of each PDF document using the '\n",
      " '`extract_text` method from the `PdfReader` class in the `PyPDF2` library. '\n",
      " 'The extracted text is then concatenated and returned as a single string.\\n'\n",
      " '\\n'\n",
      " 'Please note that you will need to have the `PyPDF2` library installed in '\n",
      " 'order to use this function.')\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(''' 'Sure! Here\\'s the code for the `get_pdf_text` function:\\n\\n```python\\ndef get_pdf_text(pdf_docs):\\n    text = \"\"\\n    for pdf in pdf_docs:\\n        pdf_reader = PdfReader(pdf)\\n        for page in pdf_reader.pages:\\n            text += page.extract_text()\\n    logger.info(\"Text Extracted...\")\\n    return text\\n```\\n\\nThis function takes a list of PDF documents as input (`pdf_docs`) and extracts the text from each page of each PDF document using the `extract_text` method from the `PdfReader` class in the `PyPDF2` library. The extracted text is then concatenated and returned as a single string.\\n\\nPlease note that you will need to have the `PyPDF2` library installed in order to use this function.''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='which function will help me to extract text from pdf ? '),\n",
       " AIMessage(content='The function that will help you extract text from PDF is the `get_pdf_text` function.'),\n",
       " HumanMessage(content='Can you share me which file and the code for get_pdf_text function'),\n",
       " AIMessage(content='Sure! Here\\'s the code for the `get_pdf_text` function:\\n\\n```python\\ndef get_pdf_text(pdf_docs):\\n    text = \"\"\\n    for pdf in pdf_docs:\\n        pdf_reader = PdfReader(pdf)\\n        for page in pdf_reader.pages:\\n            text += page.extract_text()\\n    logger.info(\"Text Extracted...\")\\n    return text\\n```\\n\\nThis function takes a list of PDF documents as input (`pdf_docs`) and extracts the text from each page of each PDF document using the `extract_text` method from the `PdfReader` class in the `PyPDF2` library. The extracted text is then concatenated and returned as a single string.\\n\\nPlease note that you will need to have the `PyPDF2` library installed in order to use this function.')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.memory.buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Which embeddings and llm model am I using to generate response ? ',\n",
       " 'chat_history': [HumanMessage(content='which function will help me to extract text from pdf ? '),\n",
       "  AIMessage(content='The function that will help you extract text from PDF is the `get_pdf_text` function.'),\n",
       "  HumanMessage(content='Can you share me which file and the code for get_pdf_text function'),\n",
       "  AIMessage(content='Sure! Here\\'s the code for the `get_pdf_text` function:\\n\\n```python\\ndef get_pdf_text(pdf_docs):\\n    text = \"\"\\n    for pdf in pdf_docs:\\n        pdf_reader = PdfReader(pdf)\\n        for page in pdf_reader.pages:\\n            text += page.extract_text()\\n    logger.info(\"Text Extracted...\")\\n    return text\\n```\\n\\nThis function takes a list of PDF documents as input (`pdf_docs`) and extracts the text from each page of each PDF document using the `extract_text` method from the `PdfReader` class in the `PyPDF2` library. The extracted text is then concatenated and returned as a single string.\\n\\nPlease note that you will need to have the `PyPDF2` library installed in order to use this function.'),\n",
       "  HumanMessage(content='Which embeddings and llm model am I using to generate response ? '),\n",
       "  AIMessage(content='The code provided uses the OpenAIEmbeddings for generating embeddings and the ChatOpenAI model for the language model (LLM).')],\n",
       " 'answer': 'The code provided uses the OpenAIEmbeddings for generating embeddings and the ChatOpenAI model for the language model (LLM).'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa('Which embeddings and llm model am I using to generate response ? ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
